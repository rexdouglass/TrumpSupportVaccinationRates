---
title: "Setup"
output: html_notebook
---


```{r}

#https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d



library(tidyverse)
library(reticulate)
library(tensorflow)

install_tensorflow(
    method               = "conda", 
    version              = "default", # Installs TF 2.0.0 (as of May 15, 2020)
    envname              = "py3.6", 
    conda_python_version = "3.6", 
    extra_packages       = c("matplotlib", "numpy", "pandas", "scikit-learn")
)

#
# To activate this environment, use
#
#     $ conda activate py3.6
#
# To deactivate an active environment, use
#
#     $ conda deactivate
conda_list()
use_condaenv("py3.6", required = TRUE)

library(tensorflow)
library(keras)


l1= 0.1^6 #6 looks like the winner
model_unpruned = keras_model_sequential() %>% 
               layer_batch_normalization() %>%
               layer_dropout(rate=0.5) %>%
               layer_dense(units=1024, activation="relu" , kernel_regularizer = regularizer_l1_l2(l1,0)) %>%  #, kernel_regularizer = regularizer_l1_l2(l1,0)
               layer_batch_normalization() %>%
               layer_dropout(rate=0.5) %>%
               layer_dense(units=1024, activation="relu" , kernel_regularizer = regularizer_l1_l2(l1,0)) %>%  #, kernel_regularizer = regularizer_l1_l2(l1,0)
               layer_batch_normalization() %>%
               layer_dropout(rate=0.5) %>%
               layer_dense(units=1024, activation="relu" , kernel_regularizer = regularizer_l1_l2(l1,0)) %>%  #, kernel_regularizer = regularizer_l1_l2(l1,0)
               layer_batch_normalization() %>%
               layer_dropout(rate=0.5) %>%
               layer_dense(units=1024, activation="relu" , kernel_regularizer = regularizer_l1_l2(l1,0)) %>%  #, kernel_regularizer = regularizer_l1_l2(l1,0)
               layer_dense(units=1, activation="linear") #we don't regularize the last layer. You can put as big a linear weight as you need on that synthetic feature

model_unpruned %>% compile( #recompile it
       loss = loss_mean_absolute_percentage_error, #"mse" ,#loss_mean_absolute_percentage_error, #"mse",
       optimizer =  optimizer_adam(lr = 0.01), #starting at 0.01 really does seem to matter
       metrics = metric_mean_absolute_percentage_error #metric_mean_pred #list("metric_mean_squared_error")
)

fit_unpruned <- model_unpruned %>% 
          fit(x=x_train_scaled[yid_train$fold!=1,] %>% Rfast::data.frame.to_matrix(col.names=T),
              y=yid_train[yid_train$fold!=1,'y'] %>% Rfast::data.frame.to_matrix(col.names=T),
              epochs = 1000,
              verbose = 1,
              batch_size=512,
              validation_data = list(x_train_scaled[yid_train$fold==1,] %>% Rfast::data.frame.to_matrix(col.names=T), 
                                     yid_train[yid_train$fold==1,'y'] %>% Rfast::data.frame.to_matrix(col.names=T)
                                     ),
              view_metrics=F, #still crashes rstudio y
              callbacks= list(callback_early_stopping(monitor = "val_loss", patience=100,
                                                      restore_best_weights=T),
                              callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience=30, verbose=T, min_lr=10^-5) #if no improvement
                              )
)

```

